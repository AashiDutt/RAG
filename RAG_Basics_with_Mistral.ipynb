{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvEeA+dCymE9EI6F9rmfDB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AashiDutt/RAG/blob/main/RAG_Basics_with_Mistral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wal5-egUjsx4"
      },
      "outputs": [],
      "source": [
        "# ! pip install faiss-cpu \"mistralai>=0.1.2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper import load_mistral_api_key\n",
        "api_key, dlai_endpoint = load_mistral_api_key(ret_key=True)"
      ],
      "metadata": {
        "id": "iWwQazYhj5zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get text"
      ],
      "metadata": {
        "id": "IBc38qorkCEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "response = requests.get(\n",
        "    \"https://www.deeplearning.ai/the-batch/a-roadmap-explores-how-ai-can-detect-and-mitigate-greenhouse-gases/\"\n",
        ")\n",
        "html_doc = response.text\n",
        "soup = BeautifulSoup(html_doc, \"html.parser\")\n",
        "tag = soup.find(\"div\", re.compile(\"^prose--styled\"))\n",
        "text = tag.text\n",
        "print(text)"
      ],
      "metadata": {
        "id": "utOXpggvj5xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save text as txt file"
      ],
      "metadata": {
        "id": "XvAK9MvmkDn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"AI_greenhouse_gas.txt\"\n",
        "with open(file_name, 'w') as file:\n",
        "    file.write(text)"
      ],
      "metadata": {
        "id": "IPP4ge-Yj5uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking"
      ],
      "metadata": {
        "id": "XSfEElc0kH7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 512\n",
        "chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]"
      ],
      "metadata": {
        "id": "5pWuV1vVj5sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "id": "5Ua-dVKPj5qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get embeddings of the chunks"
      ],
      "metadata": {
        "id": "qUF96B6skNCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from mistralai.client import MistralClient\n",
        "\n",
        "\n",
        "def get_text_embedding(txt):\n",
        "    client = MistralClient(api_key=api_key, endpoint=dlai_endpoint)\n",
        "    embeddings_batch_response = client.embeddings(model=\"mistral-embed\", input=txt)\n",
        "    return embeddings_batch_response.data[0].embedding"
      ],
      "metadata": {
        "id": "AsaB9Q2kj5oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])"
      ],
      "metadata": {
        "id": "Bes5m1Unj5l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_embeddings"
      ],
      "metadata": {
        "id": "UH_oGZ8wj5jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_embeddings[0])"
      ],
      "metadata": {
        "id": "BwthH_aJj5gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedd the user query"
      ],
      "metadata": {
        "id": "qAias2UFkWwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are the ways that AI can reduce emissions in Agriculture?\"\n",
        "question_embeddings = np.array([get_text_embedding(question)])"
      ],
      "metadata": {
        "id": "auDgVEgSj5d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_embeddings"
      ],
      "metadata": {
        "id": "S40tb5Q0j5bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# search for chunks that are similar to the query"
      ],
      "metadata": {
        "id": "zju73f_lkdaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D, I = index.search(question_embeddings, k=2)\n",
        "print(I)"
      ],
      "metadata": {
        "id": "FO5K8Hs1j5ZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
        "print(retrieved_chunk)"
      ],
      "metadata": {
        "id": "0-TKKxiYj5Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Context information is below.\n",
        "---------------------\n",
        "{retrieved_chunk}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {question}\n",
        "Answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VmSnvrNmj5UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mistralai.models.chat_completion import ChatMessage\n",
        "\n",
        "\n",
        "def mistral(user_message, model=\"mistral-small-latest\", is_json=False):\n",
        "    client = MistralClient(api_key=api_key, endpoint=dlai_endpoint)\n",
        "    messages = [ChatMessage(role=\"user\", content=user_message)]\n",
        "\n",
        "    if is_json:\n",
        "        chat_response = client.chat(\n",
        "            model=model, messages=messages, response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "    else:\n",
        "        chat_response = client.chat(model=model, messages=messages)\n",
        "\n",
        "    return chat_response.choices[0].message.content"
      ],
      "metadata": {
        "id": "5Vi1F4GVkmGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = mistral(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "hDVLd9BQkowI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG + Function calling"
      ],
      "metadata": {
        "id": "IAlgpJ6VkrmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def qa_with_context(text, question, chunk_size=512):\n",
        "    # split document into chunks\n",
        "    chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "    # load into a vector database\n",
        "    text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n",
        "    d = text_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(d)\n",
        "    index.add(text_embeddings)\n",
        "    # create embeddings for a question\n",
        "    question_embeddings = np.array([get_text_embedding(question)])\n",
        "    # retrieve similar chunks from the vector database\n",
        "    D, I = index.search(question_embeddings, k=2)\n",
        "    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
        "    # generate response based on the retrieve relevant text chunks\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Context information is below.\n",
        "    ---------------------\n",
        "    {retrieved_chunk}\n",
        "    ---------------------\n",
        "    Given the context information and not prior knowledge, answer the query.\n",
        "    Query: {question}\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    response = mistral(prompt)\n",
        "    return response"
      ],
      "metadata": {
        "id": "rVUlCP9nkotb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "I.tolist()"
      ],
      "metadata": {
        "id": "eP5WxkZmkorE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "I.tolist()[0]"
      ],
      "metadata": {
        "id": "bAA1KzTXkonj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "\n",
        "names_to_functions = {\"qa_with_context\": functools.partial(qa_with_context, text=text)}"
      ],
      "metadata": {
        "id": "NEzWdGBBkwZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below code tells model what the function is about -- helpful when we have multiple functions to pass them as a dictionary."
      ],
      "metadata": {
        "id": "BA3LJadrk2jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"qa_with_context\",\n",
        "            \"description\": \"Answer user question by retrieving relevant context\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"question\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"user question\",\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"question\"],\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "YFIUVJOtkwV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "What are the ways AI can mitigate climate change in transportation?\n",
        "\"\"\"\n",
        "\n",
        "client = MistralClient(api_key=api_key, endpoint=dlai_endpoint)\n",
        "\n",
        "response = client.chat(\n",
        "    model=\"mistral-large-latest\",\n",
        "    messages=[ChatMessage(role=\"user\", content=question)],\n",
        "    tools=tools,\n",
        "    tool_choice=\"any\", # forces function calling (default = \"auto\")\n",
        ")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "id": "NzDukLI9kwRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool_function = response.choices[0].message.tool_calls[0].function\n",
        "tool_function"
      ],
      "metadata": {
        "id": "K0gjtPWZkwOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool_function.name"
      ],
      "metadata": {
        "id": "ZyrcEltUkwLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = json.loads(tool_function.arguments)\n",
        "args"
      ],
      "metadata": {
        "id": "pv4NED0jkwH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "function_result = names_to_functions[tool_function.name](**args)\n",
        "function_result"
      ],
      "metadata": {
        "id": "Ra2hyVuIlABz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More about RAG\n",
        "To learn about more advanced chunking and retrieval methods, you can check out:\n",
        "- [Advanced Retrieval for AI with Chroma](https://learn.deeplearning.ai/courses/advanced-retrieval-for-ai/lesson/1/introduction)\n",
        "  - Sentence window retrieval\n",
        "  - Auto-merge retrieval\n",
        "- [Building and Evaluating Advanced RAG Applications](https://learn.deeplearning.ai/courses/building-evaluating-advanced-rag)\n",
        "  - Query Expansion\n",
        "  - Cross-encoder reranking\n",
        "  - Training and utilizing Embedding Adapters\n"
      ],
      "metadata": {
        "id": "kVCZKFvBkoZb"
      }
    }
  ]
}